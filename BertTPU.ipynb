{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"BertTPU.ipynb","provenance":[]},"accelerator":"TPU","widgets":{"application/vnd.jupyter.widget-state+json":{"efdc602cad45448a9d3ec09947266d46":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_26b39655a3584636aa9fc2326f499667","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_53c2f3eb9f76490a99f139415af420a5","IPY_MODEL_6762203889054eb48b54bf6530a68cf4"]}},"26b39655a3584636aa9fc2326f499667":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"53c2f3eb9f76490a99f139415af420a5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9b2b45df5ee04cc89f20e04c910172c4","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":871891,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":871891,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_11e032c833b24c8482262603bd6463e5"}},"6762203889054eb48b54bf6530a68cf4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_60e1f7bfbe3d40948a7632f78ffff5be","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 872k/872k [00:01&lt;00:00, 673kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3769ea671b484b028e4677867c1cf6e8"}},"9b2b45df5ee04cc89f20e04c910172c4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"11e032c833b24c8482262603bd6463e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"60e1f7bfbe3d40948a7632f78ffff5be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3769ea671b484b028e4677867c1cf6e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2b5fcb31a5f24d618e8fb8e7cf49c2e5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ef5ac8f62e604cc2a8b4ec1cc34562e8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d99e93673fd047eca191514077c61e47","IPY_MODEL_1fa3fd535acc44048bbf21b184ec3b07"]}},"ef5ac8f62e604cc2a8b4ec1cc34562e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d99e93673fd047eca191514077c61e47":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_eb23f636eff04cda9c830892789a0bc1","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1aeb6436276f4aa983356d4edc336cf9"}},"1fa3fd535acc44048bbf21b184ec3b07":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fa0131352a704da09f2a91ed674eded4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:00&lt;00:00, 42.3B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2cd4f8d5ad6b4d27a0254163e55bcdcc"}},"eb23f636eff04cda9c830892789a0bc1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1aeb6436276f4aa983356d4edc336cf9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fa0131352a704da09f2a91ed674eded4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2cd4f8d5ad6b4d27a0254163e55bcdcc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"18d09cc642a5433fb9176ce521602eef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0f057ad21a2d4eba8dd2d38419e46803","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_158ac5a12eaa4044a48b57255b3bc1fd","IPY_MODEL_1dd61817da31407eb9f7122be020438c"]}},"0f057ad21a2d4eba8dd2d38419e46803":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"158ac5a12eaa4044a48b57255b3bc1fd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c28185c23d444f70ba28808534c679e4","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1715180,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1715180,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e79a33af170040d3ae43517e95ac48a2"}},"1dd61817da31407eb9f7122be020438c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a7b8e513aeb64992bc10fe8329f19a00","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.72M/1.72M [00:00&lt;00:00, 4.62MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bcc7dd95910e4fd19120f3d951325de9"}},"c28185c23d444f70ba28808534c679e4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e79a33af170040d3ae43517e95ac48a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a7b8e513aeb64992bc10fe8329f19a00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bcc7dd95910e4fd19120f3d951325de9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4CVMEUe6moik","executionInfo":{"status":"ok","timestamp":1618091538654,"user_tz":-180,"elapsed":21600,"user":{"displayName":"Айдар Валиев","photoUrl":"","userId":"06889507579894137835"}},"outputId":"f3351eb3-8811-4c2d-fa74-562806aaf758"},"source":["!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting cloud-tpu-client==0.10\n","  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n","Collecting torch-xla==1.8\n","\u001b[?25l  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl (144.6MB)\n","\u001b[K     |████████████████████████████████| 144.6MB 74kB/s \n","\u001b[?25hCollecting google-api-python-client==1.8.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n","\u001b[K     |████████████████████████████████| 61kB 2.9MB/s \n","\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n","Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n","Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n","Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n","Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.2)\n","Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.28.0)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.2.8)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (4.7.2)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.4.8)\n","Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (54.2.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.12.4)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (20.9)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.53.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.4.7)\n","\u001b[31mERROR: earthengine-api 0.1.258 has requirement google-api-python-client<2,>=1.12.1, but you'll have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n","Installing collected packages: google-api-python-client, cloud-tpu-client, torch-xla\n","  Found existing installation: google-api-python-client 1.12.8\n","    Uninstalling google-api-python-client-1.12.8:\n","      Successfully uninstalled google-api-python-client-1.12.8\n","Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0 torch-xla-1.8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"trusted":true,"id":"TCxZ5AXJdIMP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618091737878,"user_tz":-180,"elapsed":50893,"user":{"displayName":"Айдар Валиев","photoUrl":"","userId":"06889507579894137835"}},"outputId":"92af1610-bdda-4bc5-baaf-3fb30f70ebd8"},"source":["!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n","!python pytorch-xla-env-setup.py --version nightly"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  5116  100  5116    0     0  39053      0 --:--:-- --:--:-- --:--:-- 39053\n","Updating... This may take around 2 minutes.\n","Updating TPU runtime to pytorch-nightly ...\n","\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\n","\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\n","Copying gs://tpu-pytorch/wheels/torch-nightly-cp37-cp37m-linux_x86_64.whl...\n","\\ [1 files][131.8 MiB/131.8 MiB]                                                \n","Operation completed over 1 objects/131.8 MiB.                                    \n","Copying gs://tpu-pytorch/wheels/torch_xla-nightly-cp37-cp37m-linux_x86_64.whl...\n","- [1 files][138.4 MiB/138.4 MiB]                                                \n","Operation completed over 1 objects/138.4 MiB.                                    \n","Copying gs://tpu-pytorch/wheels/torchvision-nightly-cp37-cp37m-linux_x86_64.whl...\n","/ [1 files][  4.9 MiB/  4.9 MiB]                                                \n","Operation completed over 1 objects/4.9 MiB.                                      \n","Processing ./torch-nightly-cp37-cp37m-linux_x86_64.whl\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==nightly) (3.7.4.3)\n","\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n","\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.9.0a0+git548765d which is incompatible.\u001b[0m\n","Installing collected packages: torch\n","Done updating TPU runtime\n","Successfully installed torch-1.9.0a0+git548765d\n","Processing ./torch_xla-nightly-cp37-cp37m-linux_x86_64.whl\n","Installing collected packages: torch-xla\n","  Found existing installation: torch-xla 1.8\n","    Uninstalling torch-xla-1.8:\n","      Successfully uninstalled torch-xla-1.8\n","Successfully installed torch-xla-1.9+223ca03\n","Processing ./torchvision-nightly-cp37-cp37m-linux_x86_64.whl\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchvision==nightly) (1.9.0a0+git548765d)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==nightly) (1.19.5)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==nightly) (7.1.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchvision==nightly) (3.7.4.3)\n","Installing collected packages: torchvision\n","Successfully installed torchvision-0.10.0a0+7da9afe\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","libomp5 is already the newest version (5.0.1-1).\n","0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"trusted":true,"id":"oo-k29P9dIMQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618092087196,"user_tz":-180,"elapsed":8759,"user":{"displayName":"Айдар Валиев","photoUrl":"","userId":"06889507579894137835"}},"outputId":"c18f621a-bee5-410a-9857-ca22d5aadb48"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.2MB 5.2MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n","\u001b[K     |████████████████████████████████| 870kB 20.3MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 42.2MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=50682b0a7730eb0b8db5af63f585dcbc086689b63ab66cf146d075dc958e161a\n","  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"trusted":true,"id":"bXTAhyQTdIMR"},"source":["import os\n","\n","os.environ['XLA_USE_BF16'] = \"1\"\n","os.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n","import random\n","import copy\n","import spacy\n","# import unidecode\n","import glob\n","import numpy as np\n","import torch\n","import pandas as pd\n","from google.colab import drive\n","from tqdm.notebook import tqdm\n","import nltk\n","from torch.utils.data import Dataset, random_split\n","from torch.utils.data.distributed import DistributedSampler\n","import re\n","# from spellchecker import SpellChecker\n","import string\n","from tqdm.notebook import tqdm\n","from sklearn.model_selection import train_test_split, TimeSeriesSplit\n","from collections import Counter, defaultdict\n","import torch.nn as nn\n","from sklearn import utils\n","import gensim.parsing.preprocessing as gsp\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import stopwords\n","# from stop_words import get_stop_words\n","from nltk.stem import WordNetLemmatizer\n","from gensim import utils\n","import gensim.parsing.preprocessing as gsp\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.pipeline import make_union\n","from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.pipeline import make_union\n","from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","# from torchtext import data, vocab, legacy\n","import torch.optim as optim\n","import logging\n","from transformers import (\n","    BertTokenizer, BertModel, BertForSequenceClassification, BertConfig,\n","    WEIGHTS_NAME, CONFIG_NAME, AdamW, get_linear_schedule_with_warmup, \n","    get_cosine_schedule_with_warmup,BertPreTrainedModel\n",")\n","from collections import deque\n","import torch_xla\n","import torch_xla.core.xla_model as xm\n","import torch_xla.distributed.parallel_loader as pl\n","import torch_xla.distributed.xla_multiprocessing as xmp\n","import torch_xla.utils.serialization as xser\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"MNPa2svddIMS"},"source":["# import torch_xla\n","# import torch_xla.core.xla_model as xm\n","# device = xm.xla_device()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9QAnHaP7Sq4_","executionInfo":{"status":"ok","timestamp":1618092114855,"user_tz":-180,"elapsed":17999,"user":{"displayName":"Айдар Валиев","photoUrl":"","userId":"06889507579894137835"}},"outputId":"7c623258-3674-4c8c-cba6-4d6ccdcb1012"},"source":["drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"trusted":true,"id":"_p9iybJ-dIMS"},"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CwbGsQ-shG7w"},"source":["class RocAucMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.y_true = np.array([0,1])\n","        self.y_pred = np.array([0.5,0.5])\n","        self.score = 0\n","\n","    def update(self, y_true, y_pred):\n","        # y_true = y_true.cpu().numpy().argmax(axis=1)\n","        y_true = y_true.cpu().numpy()\n","        y_pred = nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,1]\n","        self.y_true = np.hstack((self.y_true, y_true))\n","        self.y_pred = np.hstack((self.y_pred, y_pred))\n","        self.score = metrics.roc_auc_score(self.y_true, self.y_pred, labels=np.array([0, 1]))\n","    \n","    @property\n","    def avg(self):\n","        return self.score\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"NRW1mzNldIMS"},"source":["def clean(text):\n","  if re.findall(r'\\[\\'\\'\\]',text):\n","    return ''\n","  else:\n","    retext = re.split(',',text)\n","    clean_text = []\n","    retext[0] = retext[0][1:]\n","    retext[-1] = retext[-1][:-1]\n","    for i in range(len(retext)):\n","        retext[i] = \" \".join(retext[i].split())\n","        retext[i] = retext[i][1:-1]\n","    return '.'.join(retext)\n","\n","def clean_test(text):\n","  if re.findall(r'\\[\\'\\'\\]',text):\n","    return '[UNK]'\n","  else:\n","    retext = re.split(',',text)\n","    clean_text = []\n","    retext[0] = retext[0][1:]\n","    retext[-1] = retext[-1][:-1]\n","    for i in range(len(retext)):\n","        retext[i] = \" \".join(retext[i].split())\n","        retext[i] = retext[i][1:-1]\n","    return '.'.join(retext)\n","\n","MAX_SEQ_LENGTH = 200"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VdXpz0GdVYhg"},"source":["MAX_LEN=200\n","# class TextDataset(torch.utils.data.TensorDataset):\n","\n","#     def __init__(self, data, idxs, targets=None):\n","#         self.input_ids = data[0][idxs]\n","#         self.input_masks = data[1][idxs]\n","#         self.input_segments = data[2][idxs]\n","#         self.targets = targets[idxs] if targets is not None else np.zeros((data[0].shape[0], 1))\n","#     def __getitem__(self, idx):\n","#         input_ids =  self.input_ids[idx]\n","#         input_masks = self.input_masks[idx]\n","#         input_segments = self.input_segments[idx]\n","\n","#         target = self.targets[idx]\n","\n","#         return input_ids, input_masks, input_segments, target\n","\n","#     def __len__(self):\n","#         return len(self.input_ids)\n","\n","class TextDataset(torch.utils.data.TensorDataset):\n","\n","  def __init__(self, comment_text, targets, max_length, tokenizer, test=False):\n","    self.comment_text = comment_text\n","    self.test = test\n","    self.tokenizer = tokenizer\n","    self.max_length = max_length\n","    self.targets = targets\n","\n","  def get_tokens(self, text):\n","    encoded = tokenizer.encode_plus(\n","        text, \n","        add_special_tokens=True, \n","        max_length=self.max_length, \n","        pad_to_max_length=True\n","    )\n","    return encoded['input_ids'], encoded['attention_mask']\n","\n","  def __getitem__(self, item):\n","    \n","    text = self.comment_text[item]\n","            \n","    encoded = self.get_tokens(text)\n","    \n","    if not self.test:\n","      return torch.tensor(encoded[0],dtype=torch.long), torch.tensor(encoded[1],dtype=torch.long), torch.tensor(self.targets[item], dtype=torch.float)\n","    else:\n","      return torch.tensor(encoded[0],dtype=torch.long), torch.tensor(encoded[1])          \n","\n","  def __len__(self):\n","      return len(self.targets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"id":"kB8bVwtxIr8h","executionInfo":{"status":"error","timestamp":1618097687763,"user_tz":-180,"elapsed":352,"user":{"displayName":"Айдар Валиев","photoUrl":"","userId":"06889507579894137835"}},"outputId":"c91d4c77-e654-41ac-d813-fca4d9275120"},"source":["TextDataset(train.comment_text.values,train.toxic.values)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-14e7f66cbcd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTextDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoxic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"]}]},{"cell_type":"code","metadata":{"trusted":true,"id":"gcdeQk4xdIMT","colab":{"base_uri":"https://localhost:8080/","height":164,"referenced_widgets":["efdc602cad45448a9d3ec09947266d46","26b39655a3584636aa9fc2326f499667","53c2f3eb9f76490a99f139415af420a5","6762203889054eb48b54bf6530a68cf4","9b2b45df5ee04cc89f20e04c910172c4","11e032c833b24c8482262603bd6463e5","60e1f7bfbe3d40948a7632f78ffff5be","3769ea671b484b028e4677867c1cf6e8","2b5fcb31a5f24d618e8fb8e7cf49c2e5","ef5ac8f62e604cc2a8b4ec1cc34562e8","d99e93673fd047eca191514077c61e47","1fa3fd535acc44048bbf21b184ec3b07","eb23f636eff04cda9c830892789a0bc1","1aeb6436276f4aa983356d4edc336cf9","fa0131352a704da09f2a91ed674eded4","2cd4f8d5ad6b4d27a0254163e55bcdcc","18d09cc642a5433fb9176ce521602eef","0f057ad21a2d4eba8dd2d38419e46803","158ac5a12eaa4044a48b57255b3bc1fd","1dd61817da31407eb9f7122be020438c","c28185c23d444f70ba28808534c679e4","e79a33af170040d3ae43517e95ac48a2","a7b8e513aeb64992bc10fe8329f19a00","bcc7dd95910e4fd19120f3d951325de9"]},"executionInfo":{"status":"ok","timestamp":1618093023976,"user_tz":-180,"elapsed":2167,"user":{"displayName":"Айдар Валиев","photoUrl":"","userId":"06889507579894137835"}},"outputId":"2afc1950-eaf1-4e9b-ebdd-3e90cbec6750"},"source":["tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"efdc602cad45448a9d3ec09947266d46","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=871891.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2b5fcb31a5f24d618e8fb8e7cf49c2e5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18d09cc642a5433fb9176ce521602eef","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1715180.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"trusted":true,"id":"QQImG8yLdIMU"},"source":["# def get_bert_tokenize(df):\n","    \n","#     token_ids, segment_ids, attention_mask = np.zeros((len(df), MAX_SEQ_LENGTH)), np.zeros((len(df), MAX_SEQ_LENGTH)), np.zeros((len(df), MAX_SEQ_LENGTH))\n","    \n","#     for i, content in tqdm(enumerate(df.comment_text.values), total=len(df)):\n","        \n","#         content = tokenizer.tokenize(content)[:MAX_SEQ_LENGTH-2]\n","#         token_id = tokenizer.encode(content)\n","        \n","#         token_ids[i] = token_id + [0] * (MAX_SEQ_LENGTH-len(token_id))\n","#         segment_ids[i] = [0] * MAX_SEQ_LENGTH               \n","#         attention_mask[i] = [1] * len(token_id) + [0] * (MAX_SEQ_LENGTH - len(token_id))\n","        \n","#     return token_ids, segment_ids, attention_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"E5PoBGIhdIMU"},"source":["train = pd.read_csv('/content/drive/My Drive/Epam/train_sent_prep.csv',usecols=[\"comment_text\", \"toxic\"],\n","                    converters={'comment_text': lambda x: clean(x)})\n","# train = pd.read_csv('/content/drive/My Drive/epam/2.0/Module 2/train_sent_prep.csv')\n","\n","valid = pd.read_csv('/content/drive/My Drive/Epam/validation_sent_pr.csv', \n","                    usecols=[\"comment_text\", \"toxic\"],\n","                    converters={'comment_text': lambda x: clean(x)})\n","\n","test = pd.read_csv('/content/drive/My Drive/Epam/test_sent_pr.csv', usecols=[\"correct_text\"],\n","                    converters={'correct_text': lambda x: clean_test(x)})\n","\n","test.comment_text=test.correct_text\n","\n","# %%time\n","# x_train = get_bert_tokenize(train[train.comment_text!=''])\n","# x_test = get_bert_tokenize(test)\n","# x_validation = get_bert_tokenize(valid[valid.comment_text!=''])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aWF0cFk1O4qi","executionInfo":{"status":"ok","timestamp":1618099302372,"user_tz":-180,"elapsed":536,"user":{"displayName":"Айдар Валиев","photoUrl":"","userId":"06889507579894137835"}},"outputId":"6a2a1975-2d3c-4a84-c2d8-88d81d9912fe"},"source":[""],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gtx8Ez19jrzt"},"source":["# bert_config = BertConfig.from_pretrained('bert-base-multilingual-uncased') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4V99-nwOhQ_n"},"source":["# import pickle\n","# with open('x_train.pickle','wb') as f:\n","#   pickle.dump(x_train,f)\n","# with open('x_test.pickle','wb') as f:\n","#   pickle.dump(x_test,f)\n","# with open('x_validation.pickle','wb') as f:\n","#   pickle.dump(x_validation,f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pDNObaa-h6wZ"},"source":["# import pickle\n","# with open('/content/drive/MyDrive/Epam/x_train.pickle','rb') as f:\n","#   x_train = pickle.load(f)\n","# with open('/content/drive/MyDrive/Epam/x_test.pickle','rb') as f:\n","#   x_test = pickle.load(f)\n","# with open('/content/drive/MyDrive/Epam/x_validation.pickle','rb') as f:\n","#   x_validation = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"A3l8IgakdIMV"},"source":["# class CustomizedBert(BertPreTrainedModel):\n","  \n","#     def __init__(self, config):\n","#         super(CustomizedBert, self).__init__(config)\n","#         self.num_labels = config.num_labels\n","\n","#         self.bert = BertModel(config)\n","#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","#         self.classifier = nn.Linear(config.hidden_size*2, self.config.num_labels)\n","\n","#         self.init_weights()\n","#     def forward(\n","#         self,\n","#         input_ids=None,\n","#         attention_mask=None,\n","#         token_type_ids=None,\n","#         position_ids=None,\n","#         head_mask=None,\n","#         inputs_embeds=None,\n","#         labels=None,\n","#     ):\n","\n","#         outputs = self.bert(\n","#             input_ids,\n","#             attention_mask=attention_mask,\n","#             token_type_ids=token_type_ids,\n","#             position_ids=position_ids,\n","#             head_mask=head_mask,\n","#             inputs_embeds=inputs_embeds,\n","#         )\n","\n","#         ## mean max pooling and concatenate to a vector\n","        \n","#         avg_pool = torch.mean(outputs[0], 1)\n","#         max_pool, _ = torch.max(outputs[0], 1)\n","#         pooled_output = torch.cat((max_pool, avg_pool), 1)\n","\n","#         pooled_output = self.dropout(pooled_output)\n","#         logits = self.classifier(pooled_output)\n","\n","#         outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n","\n","#         return outputs  # (loss), logits, (hidden_states), (attentions)\n","\n","\n","# class callback:\n","#     def __init__(self):\n","#         self.score = list()\n","#         self.model = list()\n","    \n","#     def put(self, model, score):\n","#         self.score.append(score)\n","#         self.model.append(model)\n","\n","#     def get_model(self):\n","#         ind = np.argmin(self.score)\n","#         return self.model[ind]\n","    \n","\n","# def model_validation(net, val_loader, validation_length):\n","    \n","#     avg_val_loss = 0.0\n","    \n","#     net.eval()\n","#     valid_preds = np.zeros((validation_length, 1))\n","#     true_label = np.zeros((validation_length, 1))\n","    \n","#     for j, data in enumerate(val_loader):\n","\n","#         # get the inputs\n","#         input_ids, input_masks, input_segments, labels = data\n","#         pred = net(input_ids = input_ids.long().to(device),\n","#                          labels = None,\n","#                          attention_mask = input_masks.to(device),\n","#                          token_type_ids = input_segments.long().to(device)\n","#                         )[0]\n","\n","#         loss_val = loss_fn(pred, labels.float().to(device))\n","#         avg_val_loss += loss_val.item()\n","\n","#         valid_preds[j * BATCH_SIZE:(j+1) * BATCH_SIZE] = torch.sigmoid(pred).cpu().detach().numpy()\n","#         true_label[j * BATCH_SIZE:(j+1) * BATCH_SIZE]  = labels.float()\n","\n","\n","#     score = roc_auc_score(true_label, valid_preds)\n","    \n","#     return valid_preds, avg_val_loss, score\n","\n","train = train[train.comment_text!=''].reset_index(drop=True)\n","valid = valid[valid.comment_text!=''].reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mceJyOYjzvjd"},"source":["def loss_fn(outputs, targets):\n","    # pass in outputs and targets, return loss function\n","    return torch.nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"BOok_OjLdIMX"},"source":["def train_loop_fn(loader,model,device):\n","  tracker = xm.RateTracker()\n","  # cb = callback()\n","  losses = AverageMeter()\n","  auc = RocAucMeter()\n","  # start_time = time.time()\n","\n","\n","  gradient_accumulation_steps = 1\n","  # avg_loss = 0.0\n","  model.train()\n","  for step, data in enumerate(tqdm(loader)):\n","      input_ids, input_masks, labels = data\n","      pred = model(input_ids = input_ids,\n","                    attention_masks = input_masks\n","                  )\n","      # train_preds.extend(torch.sigmoid(pred).cpu().detach().numpy())\n","      # train_labels.extend(labels.float())\n","      loss = loss_fn(pred, labels.float())\n","      # avg_loss += loss.item()\n","      # loss = loss / gradient_accumulation_steps\n","      loss.backward()\n","      loss = loss.detach().item()\n","        \n","      auc.update(labels, pred)\n","      losses.update(loss, input_ids.size(0))\n","      if (step + 1) % gradient_accumulation_steps == 0:\n","          # Calling the step function on an Optimizer makes an update to its parameters\n","          xm.optimizer_step(optimizer)\n","          scheduler.step()\n","          optimizer.zero_grad()\n","\n","      tracker.add(FLAGS['batch_size'])\n","      if (step+1) % FLAGS['log_steps'] == 0:\n","          print('[xla:{}]({}) Rate={:.2f} Loss={:.4f} AUC={:.4f} GlobalRate={:.2f} Time={}'.format(\n","              xm.get_ordinal(), step+1, tracker.rate(), losses.avg, auc.avg,\n","              tracker.global_rate(), time.asctime()), flush=True)\n","  del loss\n","  del losses\n","  del outputs\n","  del input_ids\n","  del labels\n","  \n","  return losses, auc\n","\n","def valid_loop_fn(loader,model,device): \n","  fin_targets = []\n","  fin_outputs = []   \n","  losses = AverageMeter()\n","  final_scores = RocAucMeter()\n","\n","  model.eval()\n","  # valid_preds = np.zeros((validation_length, 1))\n","  # true_label = np.zeros((validation_length, 1))\n","  with torch.no_grad():\n","    for j, data in enumerate(val_loader):\n","\n","        # get the inputs\n","        input_ids, input_masks, labels = data\n","        pred = net(input_ids = input_ids.long(),\n","                          attention_mask = input_masks\n","                        )\n","\n","        loss_val = loss_fn(pred, labels.float())\n","        final_scores.update(targets, outputs)\n","        losses.update(loss.detach().item(), input_ids.size(0))\n","        del loss_val,pred,input_ids,input_masks,labels\n","\n","  return losses, final_scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vOfURx-iRzCk"},"source":["FLAGS = {}\n","FLAGS['seed'] = 1\n","FLAGS['batch_size'] = 32\n","FLAGS['num_workers'] = 0\n","FLAGS['num_epochs'] = 5\n","FLAGS['num_cores'] = 8\n","FLAGS['log_steps'] = 20"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7OpEY3LMYYDx"},"source":["# import os\n","# torch.manual_seed(FLAGS['seed'])\n","# device = xm.xla_device()\n","# gradient_accumulation_steps = 1\n","# seed_everything(FLAGS['seed'])\n","# lr = 3e-5\n","# model_list = list()\n","\n","# y_train = train.toxic.values.reshape(-1, 1)\n","# y_val = valid.toxic.values.reshape(-1, 1)\n","\n","# test_pred = np.zeros((len(test), 1))\n","\n","# test_loader = torch.utils.data.DataLoader(TextDataset(x_test, test.index),batch_size=FLAGS['batch_size'], shuffle=False)\n","# # train_dataset = TextDataset(x_train, train.index, y_train)\n","# # valid_dataset = TextDataset(x_validation, valid.index, y_val)\n","\n","# # train_sampler = DistributedSampler(train_dataset,\n","# #                                     num_replicas = xm.xrt_world_size(),\n","# #                                     rank = xm.get_ordinal(),\n","# #                                     shuffle = True)\n","# # valid_sampler = DistributedSampler(valid_dataset,\n","# #                                     num_replicas = xm.xrt_world_size(),\n","# #                                     rank = xm.get_ordinal(),\n","# #                                     shuffle = True)    \n","# ## loader\n","# train_loader = torch.utils.data.DataLoader(TextDataset(x_train, train.index, y_train),\n","#                                             batch_size=FLAGS['batch_size'], shuffle=True)\n","#                                             # num_workers = FLAGS['num_workers']\n","#                                             # sampler=train_sampler)\n","# val_loader = torch.utils.data.DataLoader(TextDataset(x_validation, valid.index, y_val),\n","#                                           batch_size=FLAGS['batch_size'], shuffle=False)\n","#                                           # num_workers = FLAGS['num_workers']\n","#                                         #  sampler=valid_sampler, \n","# # print('created loaders')\n","# t_total = len(train_loader)//gradient_accumulation_steps*FLAGS['num_epochs']\n","# warmup_proportion = 0.01\n","# num_warmup_steps = t_total * warmup_proportion\n","\n","# net = CustomizedBert.from_pretrained('bert-base-multilingual-uncased', config=bert_config)\n","# net = net.to(device)\n","# # print('Объявил модель')\n","# # loss_window = deque(maxlen = FLAGS['log_steps'])\n","# loss_fn = torch.nn.BCEWithLogitsLoss().to(device)\n","# optimizer = AdamW(net.parameters(), lr = lr)\n","# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)  # PyTorch scheduler\n","# best_valid_loss = float(\"Inf\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MY7Q0ESkkKwn"},"source":["# cb = callback()\n","\n","# SEED = 2020\n","# lr = 3e-5\n","\n","\n","# gradient_accumulation_steps = 1\n","# seed_everything(SEED)\n","# avg_loss = 0.0\n","# # train_preds = np.zeros((len(train), 1))\n","# # train_labels = np.zeros((len(train), 1))\n","# # train_preds = []\n","# # train_labels = []\n","# net.train()\n","# for step, data in enumerate(tqdm(train_loader)):\n","#     input_ids, input_masks, input_segments, labels = data\n","#     pred = net(input_ids = input_ids.long().to(device),\n","#                   labels = None,\n","#                   attention_mask = input_masks.to(device),\n","#                   token_type_ids = input_segments.long().to(device)\n","#                 )[0]\n","#     # train_preds.extend(torch.sigmoid(pred.cpu().detach()).numpy())\n","#     # train_labels.extend(labels.float())\n","#     loss = loss_fn(pred, labels.float().to(device))\n","#     avg_loss += loss.item()\n","#     loss = loss / gradient_accumulation_steps\n","#     loss.backward()\n","#     # train_preds[step * FLAGS['batch_size']:(step+1) * FLAGS['batch_size']] = torch.sigmoid(pred).cpu().detach().numpy()\n","#     # train_labels[step * FLAGS['batch_size']:(step+1) * FLAGS['batch_size']]  = labels.float()\n","    \n","#     if (step + 1) % gradient_accumulation_steps == 0:\n","#         # Calling the step function on an Optimizer makes an update to its parameters\n","#         xm.optimizer_step(optimizer, barrier=True)\n","#         scheduler.step()\n","#         optimizer.zero_grad()\n","\n","#     # tracker.add(FLAGS['batch_size'])\n","#     # if (step+1) % FLAGS['log_steps'] == 0:\n","#     #     print('[xla:{}]({}) Rate={:.2f} GlobalRate={:.2f} Time={}'.format(\n","#     #         xm.get_ordinal(), x+1, tracker.rate(),\n","#     #         tracker.global_rate(), time.asctime()), flush=True)\n","# score = roc_auc_score(train_labels, train_preds)\n","# return avg_loss/len(loader), score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"67xrl9xHHxwj"},"source":["# for step, data in enumerate(tqdm(train_loader)):\n","#     input_ids, input_masks, input_segments, labels = data\n","#     pred = net(input_ids = input_ids.long().to(device),\n","#                   labels = None,\n","#                   attention_mask = input_masks.to(device),\n","#                   token_type_ids = input_segments.long().to(device)\n","#                 )[0]\n","#     # train_preds.extend(torch.sigmoid(pred.cpu().detach()).numpy())\n","#     # train_labels.extend(labels.float())\n","#     loss = loss_fn(pred, labels.float().to(device))\n","#     avg_loss += loss.item()\n","#     loss = loss / gradient_accumulation_steps\n","#     loss.backward()\n","#     # train_preds[step * FLAGS['batch_size']:(step+1) * FLAGS['batch_size']] = torch.sigmoid(pred).cpu().detach().numpy()\n","#     # train_labels[step * FLAGS['batch_size']:(step+1) * FLAGS['batch_size']]  = labels.float()\n","    \n","#     if (step + 1) % gradient_accumulation_steps == 0:\n","#         # Calling the step function on an Optimizer makes an update to its parameters\n","#         xm.optimizer_step(optimizer, barrier=True)\n","#         scheduler.step()\n","#         optimizer.zero_grad()\n","\n","#     # tracker.add(FLAGS['batch_size'])\n","#     # if (step+1) % FLAGS['log_steps'] == 0:\n","#     #     print('[xla:{}]({}) Rate={:.2f} GlobalRate={:.2f} Time={}'.format(\n","#     #         xm.get_ordinal(), x+1, tracker.rate(),\n","#     #         tracker.global_rate(), time.asctime()), flush=True)\n","# score = roc_auc_score(train_labels, train_preds)\n","# return avg_loss/len(loader), score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":306},"id":"sgS5Ejjnrulh","executionInfo":{"status":"error","timestamp":1618067567268,"user_tz":-180,"elapsed":7680,"user":{"displayName":"Айдар Валиев","photoUrl":"","userId":"06889507579894137835"}},"outputId":"04807c7c-0f13-40f7-93a2-7f1df13897c1"},"source":["net = BertForSequenceClassification.from_pretrained('bert-base-multilingual-uncased')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-74bad019dd4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-multilingual-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m                 \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m                 raise OSError(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    580\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"HWAvqe4v5I7_"},"source":["class ToxicSimpleNNModel(nn.Module):\n","\n","    def __init__(self):\n","        super(ToxicSimpleNNModel, self).__init__()\n","        self.encoder = BertModel.from_pretrained('bert-base-multilingual-uncased')\n","        self.dropout = nn.Dropout(0.3)\n","        self.linear = nn.Linear(\n","            in_features=self.encoder.pooler.dense.out_features*2,\n","            out_features=1,\n","        )\n","\n","    def forward(self, input_ids, attention_masks):\n","        # bs, seq_length = input_ids.shape\n","        seq_x = self.encoder(input_ids=input_ids, attention_mask=attention_masks)\n","        apool = torch.mean(seq_x[0], 1)\n","        mpool, _ = torch.max(seq_x[0], 1)\n","        x = torch.cat((apool, mpool), 1)\n","        x = self.dropout(x)\n","        return self.linear(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QdDbRyCulL_k"},"source":["MX = xmp.MpModelWrapper(ToxicSimpleNNModel())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"adGSgRsVUxXO"},"source":["train = train[:60000]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UJslTRZ75uyg"},"source":["mx = ToxicSimpleNNModel()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"7f8hIMVAdIMX"},"source":["import os\n","def train_net(FLAGS):\n","  torch.manual_seed(FLAGS['seed'])\n","  seed_everything(FLAGS['seed'])\n","  lr = 3e-5\n","  model_list = list()\n","\n","  # y_train = train.toxic.values.reshape(-1, 1)\n","  # y_val = valid.toxic.values.reshape(-1, 1)\n","\n","  # test_pred = np.zeros((len(test), 1))\n","  count_proc = xm.xrt_world_size()\n","  num_proc = xm.get_ordinal()\n","  size_tr_dataset = round(len(train)/count_proc)\n","  size_val_dataset = round(len(valid)/count_proc)\n","  gradient_accumulation_steps=1\n","\n","  # test_loader = torch.utils.data.DataLoader(TextDataset(x_test, test.index),batch_size=FLAGS['batch_size'], shuffle=False)\n","  # train_dataset = TextDataset(x_train, train.index, y_train)\n","  # valid_dataset = TextDataset(x_validation, valid.index, y_val)\n","\n","  # train_sampler = DistributedSampler(train_dataset,\n","  #                                     num_replicas = xm.xrt_world_size(),\n","  #                                     rank = xm.get_ordinal(),\n","  #                                     shuffle = True)\n","  # valid_sampler = DistributedSampler(valid_dataset,\n","  #                                     num_replicas = xm.xrt_world_size(),\n","  #                                     rank = xm.get_ordinal(),\n","  #                                     shuffle = True)    \n","  ## loader\n","  train_loader = torch.utils.data.DataLoader(TextDataset(comment_text = train[num_proc*size_tr_dataset:(num_proc+1)*size_tr_dataset].comment_text.values,\n","                                                         targets=train[num_proc*size_tr_dataset:(num_proc+1)*size_tr_dataset].toxic.values, max_length=200, tokenizer=tokenizer),\n","                                             batch_size=FLAGS['batch_size'], shuffle=False,\n","                                             num_workers = FLAGS['num_workers'])\n","                                              # sampler=train_sampler)\n","  val_loader = torch.utils.data.DataLoader(TextDataset(comment_text=valid[num_proc*size_val_dataset:(num_proc+1)*size_val_dataset].comment_text.values,\n","                                                       targets=valid[num_proc*size_tr_dataset:(num_proc+1)*size_tr_dataset].toxic.values, max_length=200, tokenizer=tokenizer),\n","                                            batch_size=FLAGS['batch_size'], shuffle=False,\n","                                            num_workers = FLAGS['num_workers'])\n","                                          #  sampler=valid_sampler, \n","  print('created loaders')\n","  device = xm.xla_device()\n","  print('created device')\n","  t_total = len(train_loader)//gradient_accumulation_steps*FLAGS['num_epochs']\n","  warmup_proportion = 0.01\n","  num_warmup_steps = t_total * warmup_proportion\n","\n","  # net = CustomizedBert.from_pretrained('bert-base-multilingual-uncased', config=bert_config)\n","  model = mx.to(device)\n","  print('created_model')\n","  # loss_window = deque(maxlen = FLAGS['log_steps'])\n","  loss_fn = torch.nn.BCEWithLogitsLoss()\n","  optimizer = AdamW(model.parameters(), lr = lr)\n","  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)  # PyTorch scheduler\n","  best_valid_loss = float(\"Inf\")\n","  for epoch in range(1, FLAGS['num_epochs'] + 1):\n","    # train_loss = train_loop_fn(train_loader,net,FLAGS,device,scheduler,optimizer)\n","    # valid_loss, valid_score = valid_loop_fn(valid_loader,model,len(validation),device)\n","    # elapsed_time = time.time() - start_time \n","    # print('Epoch {}/{} \\t loss={:.4f}\\t val_loss={:.4f} \\t train_score={:.4f}\\t val_score={:.4f}\\t time={:.2f}s'\n","    #     .format(epoch, FLAGS['num_epochs'], train_loss, valid_loss, valid_score , elapsed_time))\n","    # if best_valid_loss > valid_loss:\n","    #     best_valid_loss = valid_loss\n","    #     save_checkpoint(file_path + '/' + 'net.pt', model, best_valid_loss)\n","    # print(\"Start training epoch {}\".format(epoch))\n","    xm.master_print(\"Start training epoch {}\".format(epoch))\n","    start_time = time.time()\n","    t = time.time()\n","    para_loader = pl.ParallelLoader(train_loader, [device])\n","    print('первая пошла')\n","    train_loss, train_score = train_loop_fn(para_loader.per_device_loader(device),model,device)\n","    print('Training epoch end')\n","    xm.master_print(f'[RESULT]: Train. Epoch: {epoch}, loss: {train_loss.avg:.5f}, final_score: {train_score.avg:.5f}, time: {(time.time() - t):.5f}')\n","    print(f'[RESULT]: Train. Epoch: {epoch}, loss: {train_loss.avg:.5f}, final_score: {train_score.avg:.5f}, time: {(time.time() - t):.5f}')\n","    del para_loader\n","\n","    t = time.time()\n","    para_loader = pl.ParallelLoader(valid_loader, [device])\n","    valid_loss, valid_score = valid_loop_fn(para_loader.per_device_loader(device),model,device)\n","    xm.master_print(f'[RESULT]: Validation. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n","    del para_loader\n","\n","    print('Epoch {}/{} \\t loss={:.4f}\\t val_loss={:.4f} \\t train_score={:.4f}\\t val_score={:.4f}\\t time={:.2f}s'\n","          .format(epoch, FLAGS['num_epochs'], train_loss.avg, valid_loss.avg, train_score.avg, valid_score.avg, (time.time() - start_time)))\n","    if best_valid_loss > valid_loss.avg:\n","        best_valid_loss = valid_loss.avg\n","        save_checkpoint('net.pt', model, best_valid_loss)\n","    del train_loss, valid_loss, train_score, valid_score\n","\n","    # cb.put(net, val_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"MRv5i72kdIMY"},"source":["def _mp_fn(rank, flags):\n","    FLAGS = flags\n","    torch.set_default_tensor_type('torch.FloatTensor')\n","    train_start = time.time()\n","    train_net(flags)\n","    elapsed_train_time = time.time() - train_start\n","    print(\"Process\", rank, \"finished training. Train time was:\", elapsed_train_time)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"jSiRLqdUdIMY","colab":{"base_uri":"https://localhost:8080/","height":544},"executionInfo":{"status":"error","timestamp":1618093902894,"user_tz":-180,"elapsed":3055,"user":{"displayName":"Айдар Валиев","photoUrl":"","userId":"06889507579894137835"}},"outputId":"db02533b-681a-406f-c8d8-1a8694e811e8"},"source":["FLAGS = {}\n","FLAGS['seed'] = 1\n","FLAGS['batch_size'] = 32\n","FLAGS['num_workers'] = 0\n","FLAGS['num_epochs'] = 2\n","FLAGS['num_cores'] = 8\n","FLAGS['log_steps'] = 20\n","xmp.spawn(_mp_fn, args = (FLAGS,), nprocs = FLAGS['num_cores'], start_method='fork')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["created loaders\n","created device\n","created loaders\n","created device\n","created loaders\n","created device\n","created loaders\n","created device\n","created loaders\n","created loaders\n","created device\n","created device\n"],"name":"stdout"},{"output_type":"error","ename":"ProcessExitedException","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-ee6e45415377>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_cores'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'log_steps'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mxmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_mp_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_cores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fork'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mdaemon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         start_method=start_method)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    134\u001b[0m                     \u001b[0merror_pid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfailed_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mexit_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                     \u001b[0msignal_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m                 )\n\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mProcessExitedException\u001b[0m: process 1 terminated with signal SIGKILL"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fCop7--ORvH2","executionInfo":{"status":"ok","timestamp":1617949062281,"user_tz":-180,"elapsed":7374,"user":{"displayName":"Айдар Валиев","photoUrl":"","userId":"06889507579894137835"}},"outputId":"f726299b-617c-4ad6-c754-6c981e228e16"},"source":["device=xm.xla_device()\n","net.to(device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ToxicSimpleNNModel(\n","  (encoder): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (linear): Linear(in_features=1536, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":88}]},{"cell_type":"code","metadata":{"id":"fVWcuT3O9_6h"},"source":["warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"yqOqctimdIMZ"},"source":["import re\n","import os\n","import torch\n","import pandas as pd\n","from scipy import stats\n","import numpy as np\n","\n","from tqdm import tqdm\n","from collections import OrderedDict, namedtuple\n","import torch.nn as nn\n","from torch.optim import lr_scheduler\n","import joblib\n","\n","import logging\n","import transformers\n","from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule, BertModel\n","import sys\n","from sklearn import metrics, model_selection\n","\n","import warnings\n","import torch_xla\n","import torch_xla.debug.metrics as met\n","import torch_xla.distributed.data_parallel as dp\n","import torch_xla.distributed.parallel_loader as pl\n","import torch_xla.utils.utils as xu\n","import torch_xla.core.xla_model as xm\n","import torch_xla.distributed.xla_multiprocessing as xmp\n","import torch_xla.test.test_utils as test_utils\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","class AverageMeter:\n","    \"\"\"\n","    Computes and stores the average and current value\n","    \"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","class BERTBaseUncased(nn.Module):\n","    def __init__(self, bert_path):\n","        super(BERTBaseUncased, self).__init__()\n","        self.bert_path = bert_path\n","        self.bert = transformers.BertModel.from_pretrained(self.bert_path)\n","        self.bert_drop = nn.Dropout(0.3)\n","        self.out = nn.Linear(768 * 2, 1)\n","\n","    def forward(\n","            self,\n","            ids,\n","            mask,\n","            token_type_ids\n","    ):\n","        output = self.bert(\n","            ids,\n","            attention_mask=mask,\n","            token_type_ids=token_type_ids)\n","        \n","        o1 = output[0]\n","        \n","        apool = torch.mean(o1, 1)\n","        mpool, _ = torch.max(o1, 1)\n","        cat = torch.cat((apool, mpool), 1)\n","\n","        bo = self.bert_drop(cat)\n","        p2 = self.out(bo)\n","        return p2\n","\n","\n","class BERTDatasetTraining:\n","    def __init__(self, comment_text, targets, tokenizer, max_length):\n","        self.comment_text = comment_text\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.targets = targets\n","\n","    def __len__(self):\n","        return len(self.comment_text)\n","\n","    def __getitem__(self, item):\n","        comment_text = str(self.comment_text[item])\n","        comment_text = \" \".join(comment_text.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            comment_text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","        )\n","        ids = inputs[\"input_ids\"]\n","        token_type_ids = inputs[\"token_type_ids\"]\n","        mask = inputs[\"attention_mask\"]\n","        \n","        padding_length = self.max_length - len(ids)\n","        \n","        ids = ids + ([0] * padding_length)\n","        mask = mask + ([0] * padding_length)\n","        token_type_ids = token_type_ids + ([0] * padding_length)\n","        \n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'targets': torch.tensor(self.targets[item], dtype=torch.float)\n","        }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OmSaDXe7ATuH"},"source":["class ToxicSimpleNNModel(nn.Module):\n","\n","    def __init__(self):\n","        super(ToxicSimpleNNModel, self).__init__()\n","        self.encoder = BertModel.from_pretrained('bert-base-multilingual-uncased')\n","        self.dropout = nn.Dropout(0.3)\n","        self.linear = nn.Linear(\n","            in_features=self.encoder.pooler.dense.out_features*2,\n","            out_features=1,\n","        )\n","\n","    def forward(self, input_ids, attention_masks):\n","        # bs, seq_length = input_ids.shape\n","        seq_x = self.encoder(input_ids=input_ids, attention_mask=attention_masks)\n","        apool = torch.mean(seq_x[0], 1)\n","        mpool, _ = torch.max(seq_x[0], 1)\n","        x = torch.cat((apool, mpool), 1)\n","        x = self.dropout(x)\n","        return self.linear(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D-Dbjp306nTx"},"source":["def clean(text):\n","  if re.findall(r'\\[\\'\\'\\]',text):\n","    return ''\n","  else:\n","    retext = re.split(',',text)\n","    clean_text = []\n","    retext[0] = retext[0][1:]\n","    retext[-1] = retext[-1][:-1]\n","    for i in range(len(retext)):\n","        retext[i] = \" \".join(retext[i].split())\n","        retext[i] = retext[i][1:-1]\n","    return '.'.join(retext)\n","\n","def clean_test(text):\n","  if re.findall(r'\\[\\'\\'\\]',text):\n","    return '[UNK]'\n","  else:\n","    retext = re.split(',',text)\n","    clean_text = []\n","    retext[0] = retext[0][1:]\n","    retext[-1] = retext[-1][:-1]\n","    for i in range(len(retext)):\n","        retext[i] = \" \".join(retext[i].split())\n","        retext[i] = retext[i][1:-1]\n","    return '.'.join(retext)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4evLlQce7n3G"},"source":["mx = ToxicSimpleNNModel()\n","# df_train1 = pd.read_csv(\"/content/drive/My Drive/Epam/train_sent_prep.csv\", usecols=[\"comment_text\", \"toxic\"],converters={'comment_text': lambda x: clean(x)}).fillna(\"none\")\n","# df_train1 = df_train1[df_train1.comment_text!='']\n","# df_train_full = df_train1.reset_index(drop=True)\n","# df_train = df_train_full.sample(frac=1).reset_index(drop=True)\n","\n","# df_valid = pd.read_csv('/content/drive/My Drive/Epam/validation_sent_pr.csv', usecols=[\"comment_text\", \"toxic\"], converters={'comment_text': lambda x: clean(x)})\n","# df_valid = df_valid[df_valid.comment_text!='']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DZh3aHxBBmrR"},"source":["def train_loop_fn(loader,model,device):\n","  tracker = xm.RateTracker()\n","  # cb = callback()\n","  losses = AverageMeter()\n","  auc = RocAucMeter()\n","  # start_time = time.time()\n","\n","\n","  gradient_accumulation_steps = 1\n","  # avg_loss = 0.0\n","  model.train()\n","  for step, data in enumerate(tqdm(loader)):\n","      input_ids, input_masks, labels = data\n","      pred = model(input_ids = input_ids,\n","                    attention_masks = input_masks\n","                  )\n","      # train_preds.extend(torch.sigmoid(pred).cpu().detach().numpy())\n","      # train_labels.extend(labels.float())\n","      loss = loss_fn(pred, labels.float())\n","      # avg_loss += loss.item()\n","      # loss = loss / gradient_accumulation_steps\n","      loss.backward()\n","      loss = loss.detach().item()\n","        \n","      auc.update(labels, pred)\n","      losses.update(loss, input_ids.size(0))\n","      if (step + 1) % gradient_accumulation_steps == 0:\n","          # Calling the step function on an Optimizer makes an update to its parameters\n","          xm.optimizer_step(optimizer)\n","          scheduler.step()\n","          optimizer.zero_grad()\n","\n","      tracker.add(FLAGS['batch_size'])\n","      if (step+1) % FLAGS['log_steps'] == 0:\n","          print('[xla:{}]({}) Rate={:.2f} Loss={:.4f} AUC={:.4f} GlobalRate={:.2f} Time={}'.format(\n","              xm.get_ordinal(), step+1, tracker.rate(), losses.avg, auc.avg,\n","              tracker.global_rate(), time.asctime()), flush=True)\n","  del loss\n","  del losses\n","  del outputs\n","  del input_ids\n","  del labels\n","  \n","  return losses, auc\n","\n","def valid_loop_fn(loader,model,device): \n","  fin_targets = []\n","  fin_outputs = []   \n","  losses = AverageMeter()\n","  final_scores = RocAucMeter()\n","\n","  model.eval()\n","  # valid_preds = np.zeros((validation_length, 1))\n","  # true_label = np.zeros((validation_length, 1))\n","  with torch.no_grad():\n","    for j, data in enumerate(val_loader):\n","\n","        # get the inputs\n","        input_ids, input_masks, labels = data\n","        pred = net(input_ids = input_ids.long(),\n","                          attention_mask = input_masks\n","                        )\n","\n","        loss_val = loss_fn(pred, labels.float())\n","        final_scores.update(targets, outputs)\n","        losses.update(loss.detach().item(), input_ids.size(0))\n","        del loss_val,pred,input_ids,input_masks,labels\n","\n","  return losses, final_scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2DxYdhB4AtG3"},"source":["def loss_fn(outputs, targets):\n","    return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n","\n","def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n","    model.train()\n","    for bi, d in enumerate(data_loader):\n","        ids = d[\"ids\"]\n","        mask = d[\"mask\"]\n","        token_type_ids = d[\"token_type_ids\"]\n","        targets = d[\"targets\"]\n","\n","        ids = ids.to(device, dtype=torch.long)\n","        mask = mask.to(device, dtype=torch.long)\n","        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","        targets = targets.to(device, dtype=torch.float)\n","\n","        optimizer.zero_grad()\n","        outputs = model(\n","            input_ids=ids,\n","            attention_masks=mask\n","            # token_type_ids=token_type_ids\n","        )\n","\n","        loss = loss_fn(outputs, targets)\n","        if bi % 10 == 0:\n","            xm.master_print(f'bi={bi}, loss={loss}')\n","\n","        loss.backward()\n","        xm.optimizer_step(optimizer)\n","        if scheduler is not None:\n","            scheduler.step()\n","\n","def eval_loop_fn(data_loader, model, device):\n","    model.eval()\n","    fin_targets = []\n","    fin_outputs = []\n","    for bi, d in enumerate(data_loader):\n","        ids = d[\"ids\"]\n","        mask = d[\"mask\"]\n","        token_type_ids = d[\"token_type_ids\"]\n","        targets = d[\"targets\"]\n","\n","        ids = ids.to(device, dtype=torch.long)\n","        mask = mask.to(device, dtype=torch.long)\n","        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","        targets = targets.to(device, dtype=torch.float)\n","\n","        outputs = model(\n","            input_ids=ids,\n","            attention_masks=mask\n","            # token_type_ids=token_type_ids\n","        )\n","\n","        targets_np = targets.cpu().detach().numpy().tolist()\n","        outputs_np = outputs.cpu().detach().numpy().tolist()\n","        fin_targets.extend(targets_np)\n","        fin_outputs.extend(outputs_np)    \n","\n","    return fin_outputs, fin_targets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2um7UrxM7rgE"},"source":["def _run():\n","    \n","    MAX_LEN = 192\n","    TRAIN_BATCH_SIZE = 64\n","    EPOCHS = 2\n","\n","    tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\", do_lower_case=True)\n","\n","    train_targets = df_train.toxic.values\n","    valid_targets = df_valid.toxic.values\n","\n","    train_dataset = BERTDatasetTraining(\n","        comment_text=df_train.comment_text.values,\n","        targets=train_targets,\n","        tokenizer=tokenizer,\n","        max_length=MAX_LEN\n","    )\n","\n","    train_sampler = torch.utils.data.distributed.DistributedSampler(\n","          train_dataset,\n","          num_replicas=xm.xrt_world_size(),\n","          rank=xm.get_ordinal(),\n","          shuffle=True)\n","\n","    train_data_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=TRAIN_BATCH_SIZE,\n","        sampler=train_sampler,\n","        drop_last=True,\n","        num_workers=4\n","    )\n","\n","    valid_dataset = BERTDatasetTraining(\n","        comment_text=df_valid.comment_text.values,\n","        targets=valid_targets,\n","        tokenizer=tokenizer,\n","        max_length=MAX_LEN\n","    )\n","\n","    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n","          valid_dataset,\n","          num_replicas=xm.xrt_world_size(),\n","          rank=xm.get_ordinal(),\n","          shuffle=False)\n","\n","    valid_data_loader = torch.utils.data.DataLoader(\n","        valid_dataset,\n","        batch_size=64,\n","        sampler=valid_sampler,\n","        drop_last=False,\n","        num_workers=4\n","    )\n","\n","    device = xm.xla_device()\n","    model = mx.to(device)\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n","\n","    lr = 0.4 * 1e-5 * xm.xrt_world_size()\n","    num_train_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE / xm.xrt_world_size() * EPOCHS)\n","    xm.master_print(f'num_train_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n","\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=0,\n","        num_training_steps=num_train_steps\n","    )\n","\n","    for epoch in range(EPOCHS):\n","        para_loader = pl.ParallelLoader(train_data_loader, [device])\n","        train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler=scheduler)\n","\n","        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n","        o, t = eval_loop_fn(para_loader.per_device_loader(device), model, device)\n","        xm.save(model.state_dict(), \"model.bin\")\n","        auc = metrics.roc_auc_score(np.array(t) >= 0.5, o)\n","        xm.master_print(f'AUC = {auc}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":938},"id":"3w7qN8pL8hik","executionInfo":{"status":"error","timestamp":1618095662149,"user_tz":-180,"elapsed":92465,"user":{"displayName":"Айдар Валиев","photoUrl":"","userId":"06889507579894137835"}},"outputId":"5b6b65cd-e6d9-41f5-f905-2d2c5056c8c2"},"source":["def _mp_fn(rank, flags):\n","    torch.set_default_tensor_type('torch.FloatTensor')\n","    a = _run()\n","    \n","FLAGS={}\n","xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"],"name":"stderr"},{"output_type":"stream","text":["num_train_steps = 1092, world_size=8\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"],"name":"stderr"},{"output_type":"stream","text":["bi=0, loss=0.8381646871566772\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-132d9e8cacf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mxmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_mp_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fork'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mdaemon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         start_method=start_method)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m         ready = multiprocessing.connection.wait(\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         )\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0w_Jgdkf8uWs","executionInfo":{"status":"ok","timestamp":1618264363507,"user_tz":-180,"elapsed":522,"user":{"displayName":"Айдар Валиев","photoUrl":"","userId":"06889507579894137835"}},"outputId":"4b6dc1a5-2eda-464a-edd8-37556c6a8fc3"},"source":["import random as rand\n","otd = ['Алейник','Валиев','Дудин','Миронов','Пономарев','Севергин','Сереберенников']\n","rand.choices((otd),k=2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Алейник', 'Сереберенников']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"lcSKFkV-BbXV","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1619274948377,"user_tz":-180,"elapsed":1126,"user":{"displayName":"Айдар Валиев","photoUrl":"","userId":"06889507579894137835"}},"outputId":"f9d241f1-ab5e-4fd5-f159-d3d5fea320a0"},"source":["import random as rand\n","otd = ['Дудин','Миронов','Пономарев','Севергин','Сереберенников','Сухоженко']\n","# rand.choices((otd),k=2)\n","rand.choice(otd)"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Сухоженко'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"aNfokPxGTqGU"},"source":[""],"execution_count":null,"outputs":[]}]}